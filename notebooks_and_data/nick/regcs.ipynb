{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1>Regression and Compressive Sensing</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from equadratures import *\n",
    "from equadratures.poly import vector_to_2D_grid\n",
    "import numpy as np\n",
    "from scipy.stats import linregress\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "from matplotlib.ticker import LogLocator\n",
    "from matplotlib import cm\n",
    "np.random.seed(1) # So we can check the answers\n",
    "plt.rcParams.update({'font.size': 14})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of Contents**\n",
    "\n",
    "[Example 1: Least squares regression of the Styblinski-Tang function](#eg1)\n",
    "\n",
    "[Exercise 1: Regression on the \"blade A\" dataset](#ex1)\n",
    "\n",
    "[Example 2: Compressive sensing with Styblinski-Tang function](#eg2)\n",
    "\n",
    "[Exercise 2: Compressive sensing on the \"Blade A\" dataset](#ex2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 1: Least-squares regression of the Styblinski-Tang function**<a name='eg1'></a>\n",
    "\n",
    "The 2D Styblinski-Tang function is defined as follows:\n",
    "$$f(\\mathbf{s}) = \\frac{1}{2} \\sum_{i=1}^2 s_i^4 - 16s_i^2 + 5s_i$$\n",
    "where each variable $s_1,s_2$ is uniformly distributed between $[-1,1]$. We generate 50 samples from this function with random inputs, and add a little bit of noise in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def StyblinskiTang(s):\n",
    "    n = s.size\n",
    "    f = 0\n",
    "    for i in range(n):\n",
    "        f += 0.5 * (s[i]**4 - 16.0*s[i]**2 + 5.0*s[i])\n",
    "    return f\n",
    "\n",
    "dims = 2\n",
    "N = 50\n",
    "S = np.random.uniform(-1,1,(N,dims))\n",
    "y = np.squeeze(evaluate_model(S, StyblinskiTang)) + 0.001*np.random.randn(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define the parameters and basis for this problem. We will use a quartic (degree 4) polynomial in a tensor grid basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dims = 2\n",
    "poly_order = 4\n",
    "my_params = [Parameter(poly_order, distribution='uniform',lower=-1.0, upper=1.0)\\\n",
    " for _ in range(dims)]\n",
    "my_basis = Basis('tensor-grid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can compute the coefficients by defining the polynomial model via a Poly object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_poly = Poly(my_params, my_basis, method='least-squares', sampling_args={'mesh':'user-defined',\n",
    "                                                                          'sample-points':S,\n",
    "                                                                          'sample-outputs':y})\n",
    "my_poly.set_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How good was the fit? We can query the $R^2$ of the fit on some unseen validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_valid = 20\n",
    "S_valid = np.random.uniform(-1,1,(N_valid,dims))\n",
    "y_valid = np.squeeze(evaluate_model(S_valid, StyblinskiTang))\n",
    "y_pred = np.squeeze(my_poly.get_polyfit(S_valid)) # Reshape into 1-D array\n",
    "\n",
    "R2 = linregress(y_valid, y_pred)[2]\n",
    "print(R2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the coefficients as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_array = vector_to_2D_grid(my_poly.coefficients, my_basis.elements)[2]\n",
    "plt.imshow(np.abs(coeff_array),origin='lower',cmap='gnuplot', norm=LogNorm())\n",
    "plt.colorbar(ticks=LogLocator())\n",
    "plt.xlabel('Degree in $s_1$')\n",
    "plt.ylabel('Degree in $s_2$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1: Regression on the \"blade A\" dataset.**<a name='ex1'></a>\n",
    "\n",
    "In this exercise, we study the relationship between the efficiency of a fan and the design of its blades. There are 25 design parameters that change the shape of the blade. They have been normalized such that each parameter is uniformly distributed between $[-1,1]$. We have 548 points in total, but we will reserve 48 points for testing how good our fit was. We will fit a quadratic surface with these parameters.\n",
    "\n",
    "First, let's create a list of parameters and a basis that define our problem. For the basis, use a total order basis of maximum degree 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import data\n",
    "X_train = np.loadtxt('data/bladeA_reg_training_inputs.dat') # 500 training points (you can use np.shape to check)\n",
    "X_valid = np.loadtxt('data/bladeA_reg_validation_inputs.dat') # 48 validation points\n",
    "y_train = np.loadtxt('data/bladeA_reg_training_outputs.dat')\n",
    "y_valid = np.loadtxt('data/bladeA_reg_validation_outputs.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define parameters (order 2)\n",
    "\n",
    "# Define basis (total order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can compute the coefficients via a polynomial model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the Poly class, choosing 'least-squares' as the method, and user-defined inputs/outputs.\n",
    "# Use the training data, not the validation data.\n",
    "my_poly = ...\n",
    "\n",
    "# Compute its coefficients by set_model\n",
    "\n",
    "# You can print the coefficients to see their values, or plot them with the following\n",
    "plt.scatter(range(len(my_poly.coefficients)),np.sort(np.log10(np.abs(np.squeeze(my_poly.coefficients))))[::-1])\n",
    "plt.xlabel('Sorted coefficient')\n",
    "plt.ylabel('log magnitude')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How good was the fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the fit on the validation data\n",
    "# Use linregress to calculate the R^2 of the fit\n",
    "R2 = linregress(np.squeeze(my_poly.get_polyfit(X_valid)), y_valid)[2]\n",
    "print(R2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many basis functions were used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the cardinality of the basis\n",
    "my_basis.cardinality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many coefficients are larger than 0.001 in absolute value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the values in the coefficients array that are above 0.001 in absolute value\n",
    "# Hint: use np.where and np.abs\n",
    "len(np.where(np.abs(my_poly.coefficients) > 1e-3)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 2: Compressive sensing with Styblinski-Tang function**<a name='eg2'></a>\n",
    "\n",
    "In Example 1, we saw that the coefficients of the Styblinski-Tang function were approximately sparse---there are many coefficients which are nearly zero. Hence, we repeat example 1, but now with compressive sensing, and see how many samples we can afford to save. To start off with, let's use 20. Note that this is smaller than the number of basis functions, so we need to use compressive sensing. (Alternatively, we could have reduced the number of basis functions via a total order basis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def StyblinskiTang(s):\n",
    "    n = s.size\n",
    "    f = 0\n",
    "    for i in range(n):\n",
    "        f += 0.5 * (s[i]**4 - 16.0*s[i]**2 + 5.0*s[i])\n",
    "    return f\n",
    "\n",
    "dims = 2\n",
    "N = 20\n",
    "S = np.random.uniform(-1,1,(N,dims))\n",
    "y = np.squeeze(evaluate_model(S, StyblinskiTang)) + 0.001*np.random.randn(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we define the parameters and basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dims = 2\n",
    "poly_order = 4\n",
    "my_params = [Parameter(poly_order, distribution='uniform',lower=-1.0, upper=1.0)\\\n",
    " for _ in range(dims)]\n",
    "my_basis = Basis('tensor-grid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we compute the coefficients via compressive sensing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_poly = Poly(my_params, my_basis, method='compressive-sensing', sampling_args={'mesh':'user-defined',\n",
    "               'sample-points':S,'sample-outputs':y})\n",
    "my_poly.set_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat the same exercise: Examine the goodness of fit with validation data, and then plot the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_valid = 20\n",
    "S_valid = np.random.uniform(-1,1,(N_valid,dims))\n",
    "y_valid = np.squeeze(evaluate_model(S_valid, StyblinskiTang))\n",
    "y_pred = np.squeeze(my_poly.get_polyfit(S_valid)) # Reshape into 1-D array\n",
    "\n",
    "R2 = linregress(y_valid, y_pred)[2]\n",
    "print(R2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_array = vector_to_2D_grid(my_poly.coefficients, my_basis.elements)[2]\n",
    "# Add a small quantity to ease visualization\n",
    "plt.imshow(np.abs(coeff_array)+1e-5,origin='lower',cmap='gnuplot', norm=LogNorm()) \n",
    "plt.colorbar(ticks=LogLocator())\n",
    "plt.xlabel('Degree in $s_1$')\n",
    "plt.ylabel('Degree in $s_2$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that compressive sensing tends to be less reliable. If you get a poor $R^2$, a simple remedy would be to try again with different random samples. However, if the settings are appropriate, compressive sensing can bypass the restrictions of least squares to get the same results with fewer data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2: Compressive sensing on the \"Blade A\" dataset**<a name='ex2'></a>\n",
    "\n",
    "Again, in exercise 1 we saw that only a portion of the coeffients in the approximation model for the fan efficiency are significantly large. We may be able then to reduce the number of data points we need to produce the same model. \n",
    "\n",
    "As before, we define the parameters and basis. This time, we will use 300 points and assign the rest as validation data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import data\n",
    "X_train = np.loadtxt('data/bladeA_cs_training_inputs.dat') # 300 training points (you can use np.shape to check)\n",
    "X_valid = np.loadtxt('data/bladeA_cs_validation_inputs.dat') # 248 validation points\n",
    "y_train = np.loadtxt('data/bladeA_cs_training_outputs.dat')\n",
    "y_valid = np.loadtxt('data/bladeA_cs_validation_outputs.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define parameters (order 2)\n",
    "\n",
    "# Define basis (total order)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can compute the coefficients via a polynomial model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the Poly class, choosing 'compressive-sensing' as the method, \n",
    "# and user-defined inputs/outputs.\n",
    "# Use the training data, not the validation data.\n",
    "# Set 'verbose' to True in solver_args to query the noise level used!\n",
    "my_poly = ...\n",
    "\n",
    "# Compute its coefficients by set_model\n",
    "\n",
    "# You can print the coefficients to see their values, or plot them with the following\n",
    "plt.scatter(range(len(my_poly.coefficients)),np.sort(np.log10(np.abs(np.squeeze(my_poly.coefficients))))[::-1])\n",
    "plt.xlabel('Sorted coefficient')\n",
    "plt.ylabel('log magnitude')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How good was the fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the fit on the validation data\n",
    "# Use linregress to calculate the R^2 of the fit\n",
    "R2 = linregress(np.squeeze(my_poly.get_polyfit(X_valid)), y_valid)[2]\n",
    "print(R2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What was the noise level ($\\eta$) used? Compare it with the standard deviation of the training output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the standard deviation of the training output.\n",
    "np.std(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should notice that the noise level is 0.01, which is small, but not completely negligible, compared to the standard deviation of the output. This acknowledges the fact that we have a bit of fitting error (the data is not exactly a quadratic), but the fitting error does not completely swamp the variation of the regression function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
